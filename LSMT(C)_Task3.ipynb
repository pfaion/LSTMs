{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (C) - Test Implementation and Application\n",
    "\n",
    "by Patrick Faion and Alessa Grund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "[Test](#LSTM-Cell-Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this iPython notebook we wanted to show you, how a simple LSTM Cell and a network of those cells could be implemented in Python and used for some example tasks, were the memory capacity of LSTM cells is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Cell Implementation\n",
    "\n",
    "In the following we will show our implementation for a simple LSTM cell. It is important to note some differences to the very basic ones we presented in our presentation. We choose those differences in accordance with some newer papers on optimized version of LSTMs. A very good overview about the basic architecture we used is given in the paper [Gers (2002) - Learning Precise Timing with LSTM Recurrent Networks](http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf). This paper includes the additional forget-gate and also introduces the peephole-connection between gates and cell-stated, which seem to be state of the art in many fields nowadays. Notable changes to the model we presented are:\n",
    "\n",
    "* Addition of the forget-gate.\n",
    "* Addition of peephole connections.\n",
    "* No Cell blocks anymore. Currently LSTMs are used with one set of gates for every cell.\n",
    "\n",
    "The implementation we chose here will be based on classes for all the different parts of an LSTM cell and everything will be modeled as separate objects. *This is very inefficient!* A much better way would be to accumulate everything into a few big matrices and just do linear algebra operations on them. But this hides a lot of the underlying processes, so for the sake of clarity, we chose this representation. Also we split the whole process into three parts:\n",
    "\n",
    "1. Forward pass the input.\n",
    "2. Backward pass the error.\n",
    "3. Update the weights.\n",
    "\n",
    "So every class will have three functions for the different phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "As activation functions for this model we use the sigmoid function and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Calculate the logistic sigmoid function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    \"\"\"Calculate the derivative of the logistic sigmoid function.\"\"\"\n",
    "    return sigmoid(x)*(1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gates\n",
    "\n",
    "The first thing we model is a generic class for the gates of a cell. Weights are initialized to random values in [-0.2, 0.2], as they did in the paper as well. Also we need to initialize $\\Delta W$ to store the weight differences, until the update function is called.\n",
    "\n",
    "For the biases, there will be one additional input which is constant 1. The bias weight will thus be the first weight in the weight vector. As already stated in the original LSTM-paper, different bias weights for the different gates will be beneficial, so we initilize this via a parameter.\n",
    "\n",
    "For the forward pass, we simple apply the formulas for the gates from the paper, which are identical for all gate types. See e.g. the formulas for the input gate:\n",
    "$$y_{in_j}(t) = f_{in_j}(z_{in_j}(t)) \\hspace{1cm}\\text{with}\\hspace{1cm} z_{in_j}(t) = \\sum_m w_{in_j m}y_m(t-1)$$\n",
    "The update function will also be the same for all gate types, since it will just add $\\Delta W$ to the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gate:\n",
    "    def __init__(self, inpDim, bias):\n",
    "        \"\"\"Create a gate object.\n",
    "        \n",
    "            inpDim: dimensionality of the input to the gate\n",
    "            bias:   bias for this gate\n",
    "        \"\"\"\n",
    "        # dimensionality\n",
    "        self.inpDim = inpDim\n",
    "        \n",
    "        # weight and bias\n",
    "        self.W = np.random.rand(1, self.inpDim) * 0.2 - 0.1\n",
    "        self.W[0,0] = bias\n",
    "        \n",
    "        # activation functions\n",
    "        self.f = sigmoid\n",
    "        self.f_deriv = sigmoid_deriv\n",
    "        \n",
    "        # deltaW initialization\n",
    "        self.deltaW = np.zeros(self.W.shape)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \"\"\"Forward pass an input through the gate.\n",
    "            \n",
    "            inp: the input to the gate\n",
    "        \"\"\"\n",
    "        # store input for backward pass\n",
    "        self.inp = inp\n",
    "        \n",
    "        # calculate the sum over weight * input as matrix multiplication\n",
    "        # this is the @ sign\n",
    "        self.netInp = self.W @ inp\n",
    "        \n",
    "        # apply activation function to receive output\n",
    "        self.y = self.f(self.netInp)\n",
    "        return self.y\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update weights according to stored deltaW.\"\"\"\n",
    "        self.W += self.deltaW\n",
    "        # also make sure to reset deltaW afterwards\n",
    "        self.deltaW = np.zeros(self.W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Backward pass we need to split this into three classes, since the update calculation works quite differently for the output gate, compared to the input and forget gate. The output gate can mostly calculate its gradient itself, while the input and forget gate rely on the gradient calculation by the memory cell in accordance with the memory state. We can still use the previous generic code, by creating three additional subclasses for the gate types.\n",
    "\n",
    "The backward pass formulas are:\n",
    "$$\\Delta w_{out_j m}(t) = \\alpha \\delta_{out_j}(t) y_m(t) \\hspace{1cm}\\text{with}\\hspace{1cm} \\delta_{out_j}(t) = f'_{out_j}(z_{out_j}(t)) e_{out}(t)$$\n",
    "where $e_{out}(t)$ is the incoming error into the output gate. And for input and forget gate:\n",
    "$$\\Delta w_{in_j m}(t) = \\alpha * \\text{grad}_{in_j}$$\n",
    "$$\\Delta w_{\\phi_j m}(t) = \\alpha * \\text{grad}_{\\phi_j}$$\n",
    "where the gradients have to be calculated and passed from the memory cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OutGate(Gate):\n",
    "    def backward(self, error, learningRate):\n",
    "        \"\"\"Backward pass the error and calculate weight update.\n",
    "        \n",
    "            error: the incoming error\n",
    "            learningRate: the learning rate for the weight update\n",
    "        \"\"\"\n",
    "        self.delta = self.f_deriv(self.netInp) * error\n",
    "        self.deltaW += learningRate * (self.delta @ self.inp.T)\n",
    "        \n",
    "class InpGate(Gate):\n",
    "    def backward(self, grad, learningRate):\n",
    "        \"\"\"Backward pass the error gradient and calculate weight update.\n",
    "        \n",
    "            grad: the incoming gradient\n",
    "            learningRate: the learning rate for the weight update\n",
    "        \"\"\"\n",
    "        self.deltaW += learningRate * grad\n",
    "        \n",
    "class ForgetGate(Gate):\n",
    "    def backward(self, grad, learningRate):\n",
    "        \"\"\"Backward pass the error gradient and calculate weight update.\n",
    "        \n",
    "            grad: the incoming gradient\n",
    "            learningRate: the learning rate for the weight update\n",
    "        \"\"\"\n",
    "        self.deltaW += learningRate * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cell\n",
    "\n",
    "We can now start to model an LSTM cell. Weights are also initialized to random values in [-0.2, 0.2]. We don't adjust the bias weight separately. The cell state is initialized to 0. The gates will get biases of 0, -2 and +2 respectively, which are values taken from the paper. In the end we also need to initialize the derivatives for the truncated RTRL learning algorithm.\n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "For the forward pass, the relevant formulas here are the calculation of the net cell input:\n",
    "$$ z_{c_j^v}(t) = \\sum_m w_{c_j^v m}y_m(t-1) $$\n",
    "as well as the state update:\n",
    "$$ s_{c_j^v}(t) = y_{\\phi_j}(t) s_{c_j^v}(t-1) + y_{in_j}(t) g(z_{c_j^v}(t))$$\n",
    "\n",
    "In addition, we need to update our stored derivatives:\n",
    "$$\\frac{\\partial s_{c_j^v}(t)}{\\partial w_{c_j^v m}} = \\frac{\\partial s_{c_j^v}(t-1)}{\\partial w_{c_j^v m}} y_{\\phi_j}(t) + g'(z_{c_j^v}(t)) y_{in_j}(t)y_m(t-1)$$\n",
    "$$\\frac{\\partial s_{c_j^v}(t)}{\\partial w_{in_j m}} = \\frac{\\partial s_{c_j^v}(t-1)}{\\partial w_{in_j m}} y_{\\phi_j}(t) + g(z_{c_j^v}(t)) f'_{in_j}(z_{in_j}(t)) y_m(t-1)$$\n",
    "$$\\frac{\\partial s_{c_j^v}(t)}{\\partial w_{\\phi_j m}} = \\frac{\\partial s_{c_j^v}(t-1)}{\\partial w_{\\phi_j m}} y_{\\phi_j}(t) + s_{c_j^v}(t-1) f'_{\\phi_j}(z_{\\phi_j}(t)) y_m(t-1)$$\n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "For the backward pass, we need to calculate the fraction of the error caused by the output gate and pass it on to the gate:\n",
    "$$ e_{out}(t) = \\sum_{v=1}^{S_j}s_{c_j^v}(t) e_{cell}(t) $$, where $e_{cell}$ is the error caused by this cells output. Then we calculate the cell error and weight update:\n",
    "$$e_{s_{c_j^v}}(t) = y_{out_j}(t) e_{cell}$$\n",
    "$$\\Delta w_{c_j^v m}(t) = \\alpha e_{s_{c_j^v}}(t) \\frac{\\partial S_{c_j^v}(t)}{\\partial w_{c_j^v m}}$$\n",
    "Finally calculate the gradient for input and forget gate and pass these on:\n",
    "$$\\text{grad}_{in_j} = e_{s_{c_j^v}}(t) \\frac{\\partial S_{c_j^v}(t)}{\\partial w_{in_j m}}$$\n",
    "$$\\text{grad}_{\\phi_j} = e_{s_{c_j^v}}(t) \\frac{\\partial S_{c_j^v}(t)}{\\partial w_{\\phi_j m}}$$\n",
    "\n",
    "**Update**\n",
    "\n",
    "The update function will just update the weights and reset $\\Delta W$ and subsequently call the update functions of the gate objects.\n",
    "\n",
    "**Resetting**\n",
    "\n",
    "An additional function for resetting the cell state is added in order to reset it after one training sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMCell:\n",
    "    def __init__(self, inpDim):\n",
    "        \"\"\"Create LSTM cell object.\n",
    "        \n",
    "            inpDim: dimensionality of the input\n",
    "        \"\"\"\n",
    "        \n",
    "        # --------- INIT BASIC STUFF ---------\n",
    "        # input dimensionality\n",
    "        self.inpDim = inpDim\n",
    "        \n",
    "        # state init\n",
    "        self.state = np.array([[0.0]])\n",
    "        \n",
    "        # weight matrix and deltaW init\n",
    "        self.W = np.random.rand(1, self.inpDim) * 0.2 - 0.1\n",
    "        self.deltaW = np.zeros(self.W.shape)\n",
    "        \n",
    "        # activation functions\n",
    "        self.g = sigmoid\n",
    "        self.g_deriv = sigmoid_deriv\n",
    "        \n",
    "        \n",
    "        # --------- INIT GATES ---------\n",
    "        # The gate input is one larger than cell input, because of the peephole connections.\n",
    "        gateDim = self.inpDim + 1\n",
    "        \n",
    "        # Biases for the gates were taken from the papers. They proved most successfull.\n",
    "        inpBias = 0.0\n",
    "        forgetBias = -2.0\n",
    "        outBias = 2.0\n",
    "        \n",
    "        # create the gate objects\n",
    "        self.inpGate = InpGate(gateDim, inpBias)\n",
    "        self.forgetGate = ForgetGate(gateDim, forgetBias)\n",
    "        self.outGate = OutGate(gateDim, outBias)\n",
    "        \n",
    "        \n",
    "        # --------- INIT DERIVATIVES ---------\n",
    "        self.stateDerivWRTCellWeights = np.zeros(self.W.shape)\n",
    "        self.stateDerivWRTInpGateWeights = np.zeros(self.inpGate.W.shape)\n",
    "        self.stateDerivWRTForgetGateWeights = np.zeros(self.forgetGate.W.shape)\n",
    "        \n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \"\"\"Forward pass a given input.\n",
    "        \n",
    "            inp: the input at the current time\n",
    "                IMPORTANT: remember that the first value of the input will\n",
    "                be the constant bias\n",
    "        \"\"\"\n",
    "        \n",
    "        # store input for backward pass\n",
    "        self.inp = inp\n",
    "        \n",
    "        \n",
    "        # --------- CELL INPUT ---------\n",
    "        # calculate net input to the cell by weight-multiplication\n",
    "        self.netInp = self.W @ inp\n",
    "        \n",
    "        \n",
    "        # --------- PASS TO INPUT AND FORGET GATE ---------\n",
    "        # append current state to input vector (peephole connection)\n",
    "        inpWPrevPeep = np.append(inp, self.state, axis = 0)\n",
    "        \n",
    "        # pass input with peephole connection to input- and forget-gate\n",
    "        self.inpGate.forward(inpWPrevPeep)\n",
    "        self.forgetGate.forward(inpWPrevPeep)\n",
    "        \n",
    "        \n",
    "        # --------- UPDATE DERIVATIVES ---------\n",
    "        self.stateDerivWRTCellWeights *= self.forgetGate.y\n",
    "        self.stateDerivWRTCellWeights += self.g_deriv(self.netInp) * self.inpGate.y * inp.T\n",
    "        \n",
    "        self.stateDerivWRTInpGateWeights *= self.forgetGate.y\n",
    "        self.stateDerivWRTInpGateWeights += self.g(self.netInp) * self.inpGate.f_deriv(self.inpGate.netInp) * inpWPrevPeep.T\n",
    "        \n",
    "        self.stateDerivWRTForgetGateWeights *= self.forgetGate.y\n",
    "        self.stateDerivWRTForgetGateWeights += self.state * self.forgetGate.f_deriv(self.forgetGate.netInp) * inpWPrevPeep.T\n",
    "        \n",
    "        \n",
    "        # --------- UPDATE CELL STATE ---------\n",
    "        self.state = self.forgetGate.y * self.state + self.inpGate.y * self.g(self.netInp)\n",
    "        \n",
    "        \n",
    "        # --------- PASS TO OUTPUT GATE ---------\n",
    "        # again append the (updated) cell state as peephole connection to the input\n",
    "        inpWPostPeep = np.append(inp, self.state, axis = 0)\n",
    "        self.outGate.forward(inpWPostPeep)\n",
    "        \n",
    "        # --------- CALCULATE CELL OUTPUT ---------\n",
    "        self.y = self.outGate.y * self.state\n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    \n",
    "    def backward(self, error, learningRate):\n",
    "        \"\"\"Backward pass the given error to calculate the weight update.\n",
    "        \n",
    "            error: incoming error\n",
    "            learningRate: the learning rate for the weight update\n",
    "        \"\"\"\n",
    "        \n",
    "        # --------- OUTPUT GATE ---------\n",
    "        # calculate error for output gate and pass it backward\n",
    "        outGateError = self.state * error\n",
    "        self.outGate.backward(outGateError, learningRate)\n",
    "        \n",
    "        # --------- CELL ERROR ---------\n",
    "        # calculate internal error\n",
    "        internalError = self.outGate.y * error\n",
    "        \n",
    "        # incoming weight adjustment\n",
    "        self.deltaW += learningRate * internalError * self.stateDerivWRTCellWeights\n",
    "        \n",
    "        # --------- INPUT AND FORGET GATE ---------\n",
    "        # calculate gradient for input gate and pass it backward\n",
    "        inpGateGradient = internalError * self.stateDerivWRTInpGateWeights\n",
    "        self.inpGate.backward(inpGateGradient, learningRate)\n",
    "        \n",
    "        # calculate gradient for forget gate and pass it backward\n",
    "        forgetGateGradient = internalError * self.stateDerivWRTForgetGateWeights\n",
    "        self.forgetGate.backward(forgetGateGradient, learningRate)\n",
    "        \n",
    "        \n",
    "    def update(self):\n",
    "        \"\"\"Update weights with respect to stored deltas.\"\"\"\n",
    "        # update and reset cell weights\n",
    "        self.W += self.deltaW\n",
    "        self.deltaW = np.zeros(self.W.shape)\n",
    "        \n",
    "        # update and reset gate weights\n",
    "        self.outGate.update()\n",
    "        self.inpGate.update()\n",
    "        self.forgetGate.update()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the cell state and the derivatives.\"\"\"\n",
    "        self.state = np.array([[0.0]])\n",
    "        self.stateDerivWRTCellWeights = np.zeros(self.W.shape)\n",
    "        self.stateDerivWRTInpGateWeights = np.zeros(self.inpGate.W.shape)\n",
    "        self.stateDerivWRTForgetGateWeights = np.zeros(self.forgetGate.W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Network Implementation\n",
    "\n",
    "Now we want a whole network of LSTM cells. This network will be consisting of three layers:\n",
    "\n",
    "1. Input layer\n",
    "2. Hidden layer of LSTM cells\n",
    "3. Fully connected output layer\n",
    "\n",
    "### Output Layer\n",
    "\n",
    "The input layer will not perform any computation, so we don't need to model it specifically. The hidden layer can be easily modeled with a list of LSTM cells. But we need an additional class for the output layer. This will be a layer of neurons, all fully connected to the hidden layer. We will not model them as single objects, but consider the whole layer only with one weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OutputLayer:\n",
    "    def __init__(self, inpDim, outDim):\n",
    "        \"\"\"Create an output layer.\n",
    "            \n",
    "            inpDim: dimensionality of the input\n",
    "            outDim: dimensionality of the output\n",
    "        \"\"\"\n",
    "        # init dimensions\n",
    "        self.inpDim = inpDim\n",
    "        self.outDim = outDim\n",
    "        \n",
    "        # init weights and deltaW\n",
    "        self.W = np.random.rand(self.outDim, self.inpDim) * 0.2 - 0.1\n",
    "        self.deltaW = np.zeros(self.W.shape)\n",
    "        \n",
    "        # init activation functions\n",
    "        self.f = sigmoid\n",
    "        self.f_deriv = sigmoid_deriv\n",
    "        \n",
    "    \n",
    "    def forward(self, inp):\n",
    "        \"\"\"Forward pass a given input.\n",
    "        \n",
    "            inp: the given input\n",
    "        \"\"\"\n",
    "        # store input for backward pass\n",
    "        self.inp = inp\n",
    "        \n",
    "        # calculate net input from weights\n",
    "        self.netInp = self.W @ inp\n",
    "        \n",
    "        # calculate output\n",
    "        self.y = self.f(self.netInp)\n",
    "        return self.y\n",
    "    \n",
    "    \n",
    "    def backward(self, error, learningRate):\n",
    "        \"\"\"Backward pass the error and calculate weight update.\n",
    "            \n",
    "            error: the incoming error\n",
    "            learningRate: learning rate for the weight update\n",
    "        \"\"\"\n",
    "        self.error = error\n",
    "        self.grad = self.f_deriv(self.netInp) * self.error\n",
    "        self.deltaW += learningRate * (self.grad @ self.inp.T)\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update weights with respect to stored deltaW.\"\"\"\n",
    "        self.W += self.deltaW\n",
    "        self.deltaW = np.zeros(self.W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Example Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
